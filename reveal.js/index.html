<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>OpenStack in Research</title>

		<meta name="description" content="Usage of OpenStack in scientific research">
		<meta name="author" content="Adam Huffman">

		<meta name="apple-mobile-web-app-capable" content="yes">
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/simple.css" id="theme">

		<!-- Code syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<div class="slides">
				<section>
					<h1>OpenStack in Research</h1>
					<p>
						<small>Adam Huffman <a href="http://twitter.com/adamhuffman">@adamhuffman</a></small>
					</p>
				</section>
				<section data-background="#bbbbbb">
					<h1>OpenStack in <strong>Scientific</strong> Research</h1>
					<p>
						<small>Adam Huffman <a href="http://twitter.com/adamhuffman">@adamhuffman</a></small>
					</p>
				</section>
				<section id="fragments">
					<h2>Who am I?</h2>
					<br>
					<ul>
						<li>Senior HPC and Cloud Systems Engineer at The Francis Crick Institute</li>
						<li class="fragment">Visitor at High Energy Physics Group, Imperial College London</li>
						<li class="fragment">OpenStack operator (and user) since Essex...</li>
					</ul>
					<aside class="notes">
						<p>At the Meetup in the Bluefin Building when Kevin Jackson was talking about the pain of Diablo</p>
						<p>Essex described as first usable version...</p>
					</aside>
				</section>
				<section id="fragments">
					<h2>Early cloud exposure</h2>
					<br>
					<ul>
						<li class="fragment">AWS experimentation at University of Manchester</li>
						<li class="fragment">Fedora Cloud group</li>
					</ul>
					<aside class="notes">
						<p>Avoiding internal bureaucracy for new compute/data resources</p>
						<p>Group leaders and projects "testing the waters"</p>
						<p>Credit card problem" - How big is your PI's limit?</p>
						<p>AWS experiences set context and expectations for OpenStack</p>
					</aside>
				</section>
				<section id="fragments">
					<h2>Grid Incumbents</h2>
					<ul>
						<li>Particle Physics</li>
						<li>Worldwide LHC Computing Grid</li>
						<blockquote>The mission of the WLCG project is to provide global computing resources
							to store, distribute and analyse the ~30 Petabytes (30 million Gigabytes) of
							data annually generated by the Large Hadron Collider (LHC) at CERN</blockquote>
						<li class="fragment"><em>Why bother with clouds?</em></li>
					</ul>
					<aside class="notes">
						<p>Lots of human and funding effort to develop grid infrastructure</p>
						<p>It does work</p>
						<p>Cf. Nobel Prize...</p>
					</aside>
				</section>
				<section>
					<h2>Cloud Impetus/Barriers</h2>
					<br>
					<p>WLCG not as special anymore</p>
					<p>Remote data feasibility</p>
					<br>
					<p>Inertia</p>
					<aside class="notes">
						<p>Used to be bigger than everyone else</p>
						<p>Lots of effort to maintain it, lower funding/increasing demands, Run-2, HL</p>
						<p>Hard to recruit staff/interns to work on obscure technologies</p>
						<p>Well established distributed computing frameworks, processes, habits and expectations</p>
						<p>40 gigabit for HEP group at Imperial</p>
						<p>Gain experience for later usage of commercial cloud providers</p>
					</aside>
				</section>
				<section>
					<h2>CMS</h2>
					<br>
					<p>CMS Tier0 (~9,000 cores) now on OpenStack</p>
					<p>Long-lived VMs</p>
					<p>Multicore scale testing</p>
					<p>glideinWMS</p>
					<aside class="notes">
						<p>Used to be on CERN LSF cluster</p>
						<p>Tier system for distributing and processing data</p>
						<p>Maximising efficiency by finding optimal VM size, job type etc.
						<p>Custom job scheduling system based on HTCondor</p>
					</aside>
				</section>
				<section>
					<h2>CMS High-Level Trigger</h2>
					<br>
					<p>~16,000 cores</p>
					<p>Available for repurposing</p>
					<p><strong>Must not interfere with data acquisition</strong></p>
					<aside class="notes">
						<p>Equivalent to more than worldwide CMS pledged tier 1 resources</p>
						<p>i.e. valuable computational resource</p>
						<p>When LHC not producing data e.g. LS1</p>
					</aside>
				</section>
				<section>
					<h2>CMSooooCloud</h2>
					<p>Hierarchical Squid caches</p>
					<p>Incomplete/buggy EC2 API implementation</p>
					<p>HTCondor job monitoring wrinkles</p>
					<p>Repurposed hardware</p>
					<p>Wrote a local image pre-caching tool
					<aside class="notes">
						<p>OpenStack, opportunistic, overlay, online</p>
						<p>Conditions data</p>
						<p>Nodes designed for trigger work, not clouds - no local mass storage to speak of</p>
						<p>Made case for and obtained link upgrade</p>
					</aside>
				</section>
				<section>
					<h2>A more dynamic HLT cloud?</h2>
					<p>How to expand usage?</p>
					<p>Trade-off of killing jobs versus doing useful work</p>
					<p>Need to start and stop VMs quickly and reliably</p>
					<aside class="notes">
						<p>LHC beam not always stable - do useful work in between?</p>
						<p>Randomly distributed periods of usable time</p>
						<p>Still some slow ramp-up times, cf. Tony's talk</p>
					</aside>
				</section>
				<section>
					<h2>CMS HLT and other cloud usage</h2>
					<p>Some problems with earlier OpenStack releases in stopping VMs</p>
					<p>cloud-igniter tool, to allow HLT team to be in control of VMs</p>
					<p>Also using institutional clouds - UK and Italy</p>
					<p>Commercial providers</p>
					<aside class="notes">
						<p>Cloud-igniter needed for OpenStack during fills</p>
					</aside>
				</section>
				<section>
					<h2>ATLAS</h2>
					<p>Cloud R&amp;D then production efforts</p>
					<p>Micro-CernVM with cloud-init
					<p>HTCondor batch job scheduling</p>
					<p>Cloud Scheduler dynamic VM provisioning</p>
					<p>CVMFS</p>
					<aside class="notes">
						<p>Micro-CernVM</p>
						<p>Replacing vanilla EL6 with Puppet, for contextualization</p>
						<p>Dynamic slots in HTCondor</p>
						<p>CVMFS HTTP network filesystem, primarily for software distribution</p>
					</aside>
				</section>
				<section>
					<h2>ATLAS continued</h2>
					<p>Need Squid for applications, conditions data and CVMFS - how to discover remote proxy servers? - Shoal</p>
					<p>Glint plugin for image management across multiple clouds</p>
					<p>OpenStack overheads</p>
					<p>Shift in methodology</p>
					<p>Accounting</p>
					<p>Re-use Ganglia for this</p>
					<aside class=Notes">
						<p>If actual workload is long enough, cloud disadvantages (owing to remote network access, cold CVMFS caches) aren't significant</p>
						<p>Experiment having to do monitoring of clouds, rather than relying on sites to do that - Ganglia</p>
						<p>Internal software development required to facilitate OpenStack usage</p>
						<p>pledges; provider versus consumer; need to check commercial invoices</p>
					</aside>
				</section>
				<section>
					<h2>ATLAS Opportunistic OpenStack</h2>
					<p>Simulation at Point 1 project</p>
					<blockquote>"suitable for running CPU-bound, low I/O workloads such as event generation and Monte Carlo production"</blockquote>
					<p>Similar reliability concerns as CMS</p>
					<p>Fast switching tool</p>
					<aside class="notes">
						<p>Using Icehouse, OpenStack helps decouple support for HLT/TDAQ versus simulation work</p>
					</aside>
				</section>
				<section>
					<h2>ATLAS</h2>
					<p>Interest in ARM clouds, at an early stage</p>
					<p>Would like equivalent of spot-pricing to allow backfilling on any resource</p>
					<blockquote>The distributed, multi-cloud nature of the system presents unique challenges.</blockquote>
				</section>
				<section>
					<h2>Imperial</h2>
					<p>Fun with upgrades</p>
					<p>Memory leaks in Nova</p>
					<p>Quota problems</p>
					<p>Gluster....then Ceph</p>
					<aside class="notes">
						<p>Heterogeneous hardware, started with Folsom</p>
						<p>Had to restart Nova service, a horrible hack</p>
						<p>bugzilla report still not resolved</p>
						<p>Too much manual database work</p>
						<p>Request for cloud cost comparison for GridPP</p>
					</aside>
				</section>
				<section>
					<h2>CERN</h2>
					<p>Off-site datacentre in Hungary</p>
					<p>Wrinkles with Cells</p>
					<p>Single Sign On</p>
					<p>Hypervisor tuning</p>
					<p><a href="https://cloud.exchange/en/">https://cloud.exchange/en/</a></p>
					<aside class="notes">
						<p>All new compute hardware OpenStack hypervisors</p>
						<p>Dual redundant 100Gb links to Wigner</p>
						<p>Latency concerns re. link</p>
						<p>HS06 benchmark</p>
						<p>Textbook example of benefitting from contributing to upstream</p>
						<p>DBCE, profiling work ongoing</p>
					</aside>
				</section>
				<section>
					<h2>Life Sciences</h2>
					<p>Good case for clouds</p>
					<p>Lots of software, constantly changing</p>
					<p>Hard to maintain up to date central software stacks</p>
					<p>Give users power to provision their own images</p>
					<aside class="notes">
						<p>Wild West compared with HEP</p>
					</aside>
				</section>
				<section>
					<h2>But...</h2>
					<p>Data access/data transfer</p>
					<p>Data deluge cliches</p>
				</section>
				<section>
					<h2>eMedLab</h2>
					<p>http://www.emedlab.ac.uk</p>
					<p>Consortium: Crick, UCL, QMUL, KCL, EBI, Sanger, LSHTM</p>
					<p>Medical Bioinformatics cloud, using OpenStack</p>
					<p>Exemplar of impedance mismatches between HPC and Cloud</p>
					<aside class="notes">
						<p>both at system end and user end</p>
					</aside>
				</section>
				<section>
					<h2>Other UK HEI OpenStack projects/activities</h2>
					<p>http://www.climb.ac.uk</p>
					<p>http://www.ebi.ac.uk</p>
					<p>http://www.sanger.ac.uk</p>
					<p>Cambridge, Oxford...</p>
					<p>National working groups/SIGs</p>
					<aside class="notes">
						<p>CLIMB - microbial biology, across several sites, GPFS and Ceph</p>
						<p>EBI - 1 I/O-bound, 1 compute-bound workload - bioinformatics</p>
						<p>EBI - user education/adjustment the biggest issue so far</p>
						<p>Sanger - service for external bio-incubator customers</p>
						<p>Sanger - will be extended to internal users</p>
						<p>Sanger - trade performance for reproducibility and customer workflow control</p>
					</aside>
				</section>
				<section>
					<h2>Observations</h2>
					<ul>
						<li>Research...</li>
						<li>Upgrades...</li>
						<li>Components...</li>
						<li>Logging...</li>
						<li>Wishlist...</li>
					</ul>
				</section>
				<section>
					<h2>Research Clouds</h2>
					<p>Clash of mindsets</p>
					<p>Reproducibility opportunities</p>
					<p>Orchestration</p>
				</section>
				<section>
					<h2>Upgrades...</h2>
					<p>Used to be pretty horrible</p>
					<p>Much better now</p>
					<p>Can't all use Continuous Deployment</p>
					<aside class="notes">
						<p>Database schema horrors</p>
					</aside>
				</section>
				<section>
					<h2>Components...</h2>
					<p>Modularity good in principle</p>
					<p>Can make larger system-wide changes hard e.g. global scheduler (see IRC link)</p>
					<p>Angst with Neutron, RabbitMQ, Ceilometer</p>
				</section>
				<section>
					<h2>Logging...</h2>
					<p>Signs of developer-centric thinking</p>
					<p>More granularity would be welcome</p>
				</section>
				<section>
					<h2>Wishlist...</h2>
						<p>(Continue to) Take heed of Operators</p>
						<p>Clean up APIs</p>
						<p>Containers for deployment and normal usage</p>
						<p>Manila</p>
						<aside class="notes">
							<p>Excellent work by e.g. Large Deployments Team, workshops, blueprints etc.</p>
							<p>e.g. some new APIs not properly used yet because of cross-project considerations</p>
						</aside>
				</section>
				<section>
					<h2>Thanks</h2>
					<ul>
						<li>CMS material from David Colling and Anthony Tiradani</li>
						<li>ATLAS updates from Peter Love</li>
						<li>Sanger update from James Beal</li>
					</ul>
				</section>
				<section>
					<h2>References/Links</h2>
					<ul>
						<li>CMS Diverse Use of Clouds, CHEP 2015</li>
						<li>The Evolution of Cloud Computing in ATLAS, ATL-SOFT-PROC-2015-049</li>
						<li><a href="http://cernvm.cern.ch/portal/filesystem">CVMFS</a></li>
						<li><a href="http://cernvm.cern.ch/portal/">Micro-CernVM</a></li>
						<li><a href="https://github.com/hep-gc/glint">Glint image distribution service</a></li>
					</ul>
				</section>
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>

			// Full list of configuration options available at:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				transition: 'slide', // none/fade/slide/convex/concave/zoom

				// Optional reveal.js plugins
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true },
					{ src: 'plugin/notes/notes.js', async: true }
				]
			});

		</script>

	</body>
</html>
